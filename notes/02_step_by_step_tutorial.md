# MiniMind-V â†’ Qwen3-0.6B + Cross-Attention å®Œæ”¹é€ æ•™ç¨‹

> æœ¬æ•™ç¨‹æä¾›ä»é›¶å¼€å§‹çš„è¯¦ç»†æ­¥éª¤ï¼Œæ¯ä¸€æ­¥éƒ½åŒ…å«æ½œåœ¨é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

---

## ğŸ“‹ ç›®å½•

1. [å‡†å¤‡å·¥ä½œ](#å‡†å¤‡å·¥ä½œ)
2. [é˜¶æ®µä¸€: åŸºç¡€ç¯å¢ƒéªŒè¯](#é˜¶æ®µä¸€-åŸºç¡€ç¯å¢ƒéªŒè¯)
3. [é˜¶æ®µäºŒ: ä»£ç åŸºç¡€æ”¹é€ ](#é˜¶æ®µäºŒ-ä»£ç åŸºç¡€æ”¹é€ )
4. [é˜¶æ®µä¸‰: Model Architecture ä¿®æ”¹](#é˜¶æ®µä¸‰-model-architecture-ä¿®æ”¹)
5. [é˜¶æ®µå››: è®­ç»ƒé€»è¾‘é€‚é…](#é˜¶æ®µå››-è®­ç»ƒé€»è¾‘é€‚é…)
6. [é˜¶æ®µäº”: é¦–æ¬¡æµ‹è¯•è¿è¡Œ](#é˜¶æ®µäº”-é¦–æ¬¡æµ‹è¯•è¿è¡Œ)
7. [é˜¶æ®µå…­: æ­£å¼è®­ç»ƒ](#é˜¶æ®µå…­-æ­£å¼è®­ç»ƒ)
8. [æ•…éšœæ’æŸ¥æ±‡æ€»](#æ•…éšœæ’æŸ¥æ±‡æ€»)

---

## å‡†å¤‡å·¥ä½œ

### 0.1 æ£€æŸ¥ç›®å½•ç»“æ„

```bash
cd C:\Users\z1272\Desktop\LLM_Projects\minimind-v
tree /F /A
```

**é¢„æœŸç»“æ„**:
```
minimind-v/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ model_vlm.py          â­ ä¸»è¦ä¿®æ”¹
â”‚   â”œâ”€â”€ model_minimind.py
â”‚   â””â”€â”€ vision_model/
â”œâ”€â”€ trainer/
â”‚   â”œâ”€â”€ train_pretrain_vlm.py
â”‚   â”œâ”€â”€ train_sft_vlm.py
â”‚   â””â”€â”€ trainer_utils.py      â­ éœ€è¦ä¿®æ”¹
â”œâ”€â”€ dataset/
â”œâ”€â”€ out/                      (æ¨¡å‹ä¿å­˜ç›®å½•)
â”œâ”€â”€ checkpoints/              (è®­ç»ƒæ£€æŸ¥ç‚¹)
â””â”€â”€ notes/
```

**é—®é¢˜**: æ‰¾ä¸åˆ° Qwen3 æƒé‡ï¼Ÿ
```bash
# è§£å†³æ–¹æ¡ˆ
ls -la ../Models/Qwen3-0.6B/model.safetensors
# å¦‚æœä¸å­˜åœ¨ï¼Œè¯´æ˜æƒé‡è·¯å¾„é”™è¯¯ï¼Œä¿®å¤ä½ çš„è·¯å¾„
```

---

### 0.2 ç¯å¢ƒä¾èµ–æ£€æŸ¥

```bash
# æ¿€æ´»ä½ çš„ Python ç¯å¢ƒ
conda activateyour_env_name  # æˆ– source venv/bin/activate

# æ£€æŸ¥å…³é”®ä¾èµ–
python -c "import torch; print('torch:', torch.__version__)"
python -c "import transformers; print('transformers:', transformers.__version__)"
python -c "import safetensors; print('safetensors ok')"
```

**é—®é¢˜**: `ModuleNotFoundError: No module named 'safetensors'`
```bash
# è§£å†³æ–¹æ¡ˆ
pip install safetensors
```

**é—®é¢˜**: `transformers` ç‰ˆæœ¬å¤ªä½ä¸æ”¯æŒ Qwen3
```bash
# è§£å†³æ–¹æ¡ˆ
pip install transformers>=4.35.0
```

---

## é˜¶æ®µä¸€: åŸºç¡€ç¯å¢ƒéªŒè¯

### Step 1.1: éªŒè¯ Qwen3 æ¨¡å‹åŠ è½½

åˆ›å»ºæµ‹è¯•è„šæœ¬ `notes/test_qwen3_loading.py`:

```python
import torch
from transformers import Qwen3ForCausalLM, Qwen3Config

print("="*50)
print("Step 1.1: éªŒè¯ Qwen3 æ¨¡å‹åŠ è½½")
print("="*50)

# æµ‹è¯•åŠ è½½
try:
    model_path = "../Models/Qwen3-0.6B"
    config = Qwen3Config.from_pretrained(model_path)
    print(f"âœ“ Config loaded")
    print(f"  - hidden_size: {config.hidden_size}")
    print(f"  - num_hidden_layers: {config.num_hidden_layers}")
    print(f"  - num_attention_heads: {config.num_attention_heads}")
    print(f"  - vocab_size: {config.vocab_size}")
    
    model = Qwen3ForCausalLM.from_pretrained(model_path)
    print(f"âœ“ Model loaded")
    print(f"  - Parameter count: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_ids = torch.randint(0, config.vocab_size, (1, 10))
    outputs = model(input_ids)
    print(f"âœ“ Forward pass works")
    print(f"  - Output shape: {outputs.logits.shape}")
    
    print("âœ“ Step 1.1 PASSED")
    
except Exception as e:
    print(f"âœ— Step 1.1 FAILED: {e}")
    raise
```

è¿è¡Œ:
```bash
cd notes
python test_qwen3_loading.py
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| `FileNotFoundError` | è·¯å¾„é”™è¯¯ | æ£€æŸ¥ `../Models/Qwen3-0.6B` æ˜¯å¦å­˜åœ¨ |
| `torch.cuda.OutOfMemoryError` | æ˜¾å­˜ä¸è¶³ | ä½¿ç”¨ `device_map="cpu"` æˆ–å‡å°‘æ‰¹æ¬¡å¤§å° |
| `ImportError` | transformers ç‰ˆæœ¬ä½ | `pip install --upgrade transformers` |
| `KeyError: model.safetensors` | æƒé‡ç¼ºå¤± | æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰ `model.safetensors` æ–‡ä»¶ |

---

### Step 1.2: éªŒè¯ Vision Encoder (CLIP)

åˆ›å»ºæµ‹è¯•è„šæœ¬ `notes/test_clip_loading.py`:

```python
import torch
from transformers import CLIPModel, CLIPProcessor

print("="*50)
print("Step 1.2: éªŒè¯ CLIP æ¨¡å‹åŠ è½½")
print("="*50)

try:
    model_path = "../model/vision_model/clip-vit-base-patch16"
    
    model = CLIPModel.from_pretrained(model_path)
    processor = CLIPProcessor.from_pretrained(model_path)
    
    print(f"âœ“ CLIP model loaded")
    
    # å†»ç»“å‚æ•°æµ‹è¯•
    for param in model.parameters():
        param.requires_grad = False
    
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ“ Parameters frozen (trainable: {trainable})")
    
    # æµ‹è¯•å›¾åƒç¼–ç 
    from PIL import Image
    import numpy as np
    
    # åˆ›å»ºéšæœºå›¾åƒ
    img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
    inputs = processor(images=img, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.vision_model(**inputs)
    
    print(f"âœ“ Image encoding works")
    print(f"  - Output shape: {outputs.last_hidden_state.shape}")  # [1, 197, 768]
    print(f"  - Patch tokens shape: {outputs.last_hidden_state[:, 1:, :].shape}")  # [1, 196, 768]
    
    print("âœ“ Step 1.2 PASSED")
    
except Exception as e:
    print(f"âœ— Step 1.2 FAILED: {e}")
    raise
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| CLIP æ¨¡å‹æ–‡ä»¶å¤¹ä¸å­˜åœ¨ | æœªä¸‹è½½ | è¿è¡Œ README ä¸­çš„ git clone å‘½ä»¤ |
| `ModuleNotFoundError` | ç¼ºå°‘ä¾èµ– | `pip install transformers pillow` |
| å›¾åƒå¤„ç†å¤±è´¥ | PIL Image çš„é—®é¢˜ | ç¡®ä¿ `from PIL import Image` æ­£å¸¸ |

---

### Step 1.3: éªŒè¯ safetensors æ¥å£

åˆ›å»ºæµ‹è¯•è„šæœ¬ `notes/test_safetensors.py`:

```python
import torch
from safetensors import safe_open

print("="*50)
print("Step 1.3: éªŒè¯ safetensors æ¥å£")
print("="*50)

try:
    weight_path = "../Models/Qwen3-0.6B/model.safetensors"
    
    # æ–¹å¼ 1: åŠ è½½æ‰€æœ‰æƒé‡
    print("\n[æ–¹å¼ 1] åŠ è½½æ‰€æœ‰æƒé‡...")
    state_dict = {}
    with safe_open(weight_path, framework='pt', device='cpu') as f:
        keys = list(f.keys())
        print(f"  æ€» key æ•°é‡: {len(keys)}")
        print(f"  ç¤ºä¾‹ keys: {keys[:5]}")
        
        for key in keys:
            state_dict[key] = f.get_tensor(key)
    
    print(f"âœ“ åŠ è½½å®Œæˆ, å…± {len(state_dict)} ä¸ªå¼ é‡")
    
    # æ–¹å¼ 2: æ‡’åŠ è½½å•ä¸ªå¼ é‡
    print("\n[æ–¹å¼ 2] æ‡’åŠ è½½å•ä¸ªå¼ é‡...")
    with safe_open(weight_path, framework='pt', device='cpu') as f:
        embed_weight = f.get_tensor('model.embed_tokens.weight')
        print(f"  model.embed_tokens.weight shape: {embed_weight.shape}")
    
    # æ–¹å¼ 3: æ‡’åŠ è½½ç‰‡æ®µ (é€‚ç”¨äºå¤§æ¨¡å‹)
    print("\n[æ–¹å¼ 3] æ‡’åŠ è½½ç‰‡æ®µ...")
    with safe_open(weight_path, framework='pt', device='cpu') as f:
        embed_slice = f.get_slice('model.embed_tokens.weight')
        vocab_size, hidden_dim = embed_slice.get_shape()
        print(f"  vocab_size: {vocab_size}, hidden_dim: {hidden_dim}")
        
        # åªåŠ è½½å‰ 1000 ä¸ª token
        partial = embed_slice[:1000, :]
        print(f"  éƒ¨åˆ†åŠ è½½ shape: {partial.shape}")
    
    print("âœ“ Step 1.3 PASSED")
    
except Exception as e:
    print(f"âœ— Step 1.3 FAILED: {e}")
    import traceback
    traceback.print_exc()
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| `FileNotFoundError` | æƒé‡è·¯å¾„é”™è¯¯ | æ£€æŸ¥ `../Models/Qwen3-0.6B/model.safetensors` æ˜¯å¦å­˜åœ¨ |
| `OSError` | æ–‡ä»¶æ ¼å¼é”™è¯¯ | é‡æ–°ä¸‹è½½ safetensors æ–‡ä»¶ |
| æƒé‡åŠ è½½é¡ºåºé—®é¢˜ | æ—  | safetensors ä¸æ”¯æŒéšæœºè®¿é—®ï¼Œé¡ºåºè¯»å–å³å¯ |

---

## é˜¶æ®µäºŒ: ä»£ç åŸºç¡€æ”¹é€ 

### Step 2.1: å¤‡ä»½åŸå§‹ä»£ç 

```bash
# åœ¨ notes ç›®å½•åˆ›å»ºå¤‡ä»½
cd notes
mkdir backup_$(date +%Y%m%d_%H%M%S)
cp ../model/model_vlm.py backup_*/
cp ../trainer/trainer_utils.py backup_*/
echo "âœ“ åŸå§‹æ–‡ä»¶å·²å¤‡ä»½åˆ° notes/backup_*/"
```

---

### Step 2.2: ä¿®æ”¹ VisionProj â†’ CrossAttentionProjector

æ‰“å¼€ `model/model_vlm.py`ï¼Œå®šä½åˆ°ç¬¬ 26-37 è¡Œ:

**ç¼–è¾‘æ­¥éª¤**:

1. æ‰¾åˆ°åŸæœ‰çš„ `VisionProj` ç±» (26-37 è¡Œ)
2. åˆ é™¤è¯¥ç±»
3. åœ¨ç›¸åŒä½ç½®æ’å…¥ä»¥ä¸‹ä»£ç :

```python
class CrossAttentionProjector(nn.Module):
    def __init__(self, ve_hidden_size=768, hidden_size=1024, num_heads=8, num_layers=2):
        super().__init__()
        self.ve_hidden_size = ve_hidden_size
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        
        self.vision_adapter = nn.Linear(ve_hidden_size, hidden_size)
        
        self.cross_attn_layers = nn.ModuleList([
            self._make_cross_attn_layer(hidden_size, num_heads)
            for _ in range(num_layers)
        ])
        
        self.output_norm = nn.LayerNorm(hidden_size)
        self._init_weights()
    
    def _make_cross_attn_layer(self, hidden_size, num_heads):
        return nn.ModuleDict({
            'cross_attn': nn.MultiheadAttention(
                embed_dim=hidden_size,
                num_heads=num_heads,
                batch_first=True,
                dropout=0.1
            ),
            'norm1': nn.LayerNorm(hidden_size),
            'norm2': nn.LayerNorm(hidden_size),
            'ffn': nn.Sequential(
                nn.Linear(hidden_size, hidden_size * 4),
                nn.GELU(),
                nn.Linear(hidden_size * 4, hidden_size),
                nn.Dropout(0.1)
            )
        })
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.trunc_normal_(module.weight, std=0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, vision_features, text_features, image_indices=None):
        batch_size = text_features.shape[0]
        vision_proj = self.vision_adapter(vision_features)
        
        if image_indices:
            output_features = text_features.clone()
            for batch_idx, positions in image_indices.items():
                for img_idx, (start, end) in enumerate(positions):
                    query = text_features[batch_idx, start:end+1, :]
                    
                    if img_idx < vision_proj.shape[1]:
                        key_value = vision_proj[batch_idx:batch_idx+1, img_idx:img_idx+1, :]
                        
                        attn_out = query
                        for layer in self.cross_attn_layers:
                            attn_out, _ = layer['cross_attn'](
                                attn_out.unsqueeze(0), key_value, key_value
                            )
                            attn_out = attn_out.squeeze(0)
                            attn_out = layer['norm1'](attn_out + query)
                            ffn_out = layer['ffn'](attn_out)
                            attn_out = layer['norm2'](attn_out + ffn_out)
                        
                        output_features[batch_idx, start:end+1, :] = attn_out
            
            return self.output_norm(output_features)
        else:
            return self.output_norm(vision_proj)
```

**éªŒè¯ç¼–è¯‘**: ä¿å­˜æ–‡ä»¶ï¼Œç¡®ä¿æ²¡æœ‰è¯­æ³•é”™è¯¯ã€‚

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `IndentationError` | æ£€æŸ¥ç¼©è¿›ï¼Œç¡®ä¿ä½¿ç”¨ 4 ç©ºæ ¼ |
| `NameError: name 'nn' is not defined` | ç¡®ä¿ `from torch import nn` åœ¨æ–‡ä»¶é¡¶éƒ¨ |
| æ‰¾ä¸åˆ°åŸå§‹è¡Œå· | ä½¿ç”¨ç¼–è¾‘å™¨çš„æœç´¢åŠŸèƒ½æœç´¢ `class VisionProj` |

---

### Step 2.3: ä¿®æ”¹ MiniMindVLM ç±»ç»§æ‰¿

ç»§ç»­ç¼–è¾‘ `model/model_vlm.py`ï¼Œå®šä½åˆ°ç¬¬ 41 è¡Œ:

**ä¿®æ”¹å‰**:
```python
from .model_minimind import *
```

**ä¿®æ”¹å**:
```python
from .model_minimind import *
from transformers import Qwen3ForCausalLM, Qwen3Config
```

ç„¶åä¿®æ”¹ `MiniMindVLM` ç±» (41-49 è¡Œ):

**åŸä»£ç **:
```python
class MiniMindVLM(MiniMindForCausalLM):
    config_class = VLMConfig

    def __init__(self, params: VLMConfig = None, vision_model_path="./model/vision_model/clip-vit-base-patch16"):
        super().__init__(params)
        if not params: params = VLMConfig()
        self.params = params
        self.vision_encoder, self.processor = self.__class__.get_vision_model(vision_model_path)
        self.vision_proj = VisionProj(hidden_size=params.hidden_size)
```

**æ›¿æ¢ä¸º**:
```python
class MiniMindVLM(Qwen3ForCausalLM):
    config_class = VLMConfig

    def __init__(self, params: VLMConfig = None, 
                 vision_model_path="./model/vision_model/clip-vit-base-patch16",
                 qwen_weight_path="../Models/Qwen3-0.6B"):
        if params is None:
            params = VLMConfig()
        self.params = params
        
        qwen_config = Qwen3Config.from_pretrained(qwen_weight_path)
        super().__init__(qwen_config)
        
        self.vision_encoder, self.processor = self.__class__.get_vision_model(vision_model_path)
        
        self.vision_proj = CrossAttentionProjector(
            ve_hidden_size=768,
            hidden_size=qwen_config.hidden_size,
            num_heads=8,
            num_layers=2
        )
        
        self._load_qwen_weights(qwen_weight_path)
    
    def _load_qwen_weights(self, weight_path: str):
        import os
        weight_file = os.path.join(weight_path, "model.safetensors")
        
        if os.path.exists(weight_file):
            from safetensors import safe_open
            state_dict = {}
            with safe_open(weight_file, framework="pt", device="cpu") as f:
                for key in f.keys():
                    state_dict[key] = f.get_tensor(key)
            
            model_state = self.state_dict()
            qwen_keys = [k for k in model_state.keys() if not k.startswith('vision_proj')]
            qwen_state = {k: v for k, v in state_dict.items() if k in qwen_keys}
            
            miss, unexpected = self.load_state_dict(state_dict, strict=False)
            if miss:
                print(f"[Warning] Missing keys: {len(miss)}")
            if unexpected:
                print(f"[Warning] Unexpected keys: {len(unexpected)}")
        else:
            print(f"[Warning] Qwen3 weight not found: {weight_file}")
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `NameError: name 'Qwen3ForCausalLM' is not defined` | ç¡®ä¿å¯¼å…¥è¯­å¥åœ¨ç±»å®šä¹‰å‰ |
| `ImportError: cannot import name 'Qwen3ForCausalLM'` | æ£€æŸ¥ transformers ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ Qwen3 |
| `FileNotFoundError` | æ£€æŸ¥ `qwen_weight_path` æ˜¯å¦æ­£ç¡® |
| `RuntimeError: Error(s) in loading state_dict` | å¯èƒ½æ˜¯ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨ `strict=False` |

---

### Step 2.4: ä¿®æ”¹ count_vision_proj æ–¹æ³•

ç»§ç»­ç¼–è¾‘ `model/model_vlm.py`ï¼Œæ‰¾åˆ° `count_vision_proj` æ–¹æ³• (77-110 è¡Œ):

**åŸä»£ç **:
```python
def count_vision_proj(self, tokens, h, vision_tensors=None, seqlen=512):
    # ... åŸæœ‰é€»è¾‘
    if vision_tensors is not None and image_indices:
        vision_proj = self.vision_proj(vision_tensors)
        # ... çº¿æ€§æŠ•å½±æ›¿æ¢
```

**ä¿®æ”¹ä¸º**:
```python
def count_vision_proj(self, tokens, h, vision_tensors=None, seqlen=512):
    def find_indices(tokens, image_ids):
        image_ids_tensor = torch.tensor(image_ids).to(tokens.device)
        len_image_ids = len(image_ids)
        if len_image_ids > tokens.size(1):
            return None
        tokens_view = tokens.unfold(1, len_image_ids, 1)
        matches = (tokens_view == image_ids_tensor).all(dim=2)
        return {
            batch_idx: [(idx.item(), idx.item() + len_image_ids - 1) for idx in
                        matches[batch_idx].nonzero(as_tuple=True)[0]]
            for batch_idx in range(tokens.size(0)) if matches[batch_idx].any()
        } or None

    image_indices = find_indices(tokens, self.params.image_ids)
    if vision_tensors is not None and image_indices:
        h = self.vision_proj(vision_tensors, h, image_indices)
        return h[:, :seqlen]
    return h
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| æ–¹æ³•ç­¾åä¸ `forward` è°ƒç”¨ä¸åŒ¹é… | ä»”ç»†æ£€æŸ¥ `forward` æ–¹æ³•ä¸­ `count_vision_proj` çš„è°ƒç”¨ |
| `TypeError: 'CrossAttentionProjector' object is not subscriptable` | æ£€æŸ¥ `vision_proj.forward()` è¿”å›å€¼ç±»å‹ |
| `IndexError` | ç¡®ä¿ `image_indices` å’Œ `vision_tensors` çš„å½¢çŠ¶åŒ¹é… |

---

### Step 2.5: æ›´æ–° VLMConfig

ç¼–è¾‘ `model/model_minimind.py`ï¼Œæ‰¾åˆ° `VLMConfig` ç±» (13-24 è¡Œ):

**ä¿®æ”¹ä¸º**:
```python
class VLMConfig(MiniMindConfig):
    model_type = "minimind-v"

    def __init__(
            self,
            image_special_token: str = '@' * 196,
            image_ids: List = [34] * 196,
            hidden_size: int = 1024,
            num_hidden_layers: int = 28,
            num_attention_heads: int = 16,
            num_key_value_heads: int = 8,
            vocab_size: int = 151936,
            max_position_embeddings: int = 40960,
            rope_theta: float = 1000000.0,
            **kwargs,
    ):
        self.image_special_token = image_special_token
        self.image_ids = image_ids
        super().__init__(
            hidden_size=hidden_size,
            num_hidden_layers=num_hidden_layers,
            num_attention_heads=num_attention_heads,
            num_key_value_heads=num_key_value_heads,
            vocab_size=vocab_size,
            max_position_embeddings=max_position_embeddings,
            rope_theta=rope_theta,
            **kwargs
        )
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `TypeError: __init__() got an unexpected keyword argument` | æ£€æŸ¥ `MiniMindConfig` æ˜¯å¦æ”¯æŒè¿™äº›å‚æ•° |
| å‚æ•°é»˜è®¤å€¼å†²çª | ç¡®ä¿ Qwen3 å‚æ•°ä¸ `MiniMindConfig` ä¸å†²çª |

---

## é˜¶æ®µä¸‰: Model Architecture ä¿®æ”¹

### Step 3.1: æµ‹è¯•æ¨¡å‹å®ä¾‹åŒ–

åˆ›å»ºæµ‹è¯•è„šæœ¬ `notes/test_model_instance.py`:

```python
import torch
import sys
sys.path.insert(0, '..')
from model.model_vlm import MiniMindVLM, VLMConfig

print("="*50)
print("Step 3.1: æµ‹è¯•æ¨¡å‹å®ä¾‹åŒ–")
print("="*50)

try:
    config = VLMConfig(
        hidden_size=1024,
        num_hidden_layers=28,
        num_attention_heads=16,
        num_key_value_heads=8
    )
    
    print(f"âœ“ Config created")
    
    model = MiniMindVLM(
        params=config,
        vision_model_path="../model/vision_model/clip-vit-base-patch16",
        qwen_weight_path="../Models/Qwen3-0.6B"
    )
    
    print(f"âœ“ Model instantiated")
    
    param_count = sum(p.numel() for p in model.parameters()) / 1e6
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    
    print(f"  - Total params: {param_count:.2f}M")
    print(f"  - Trainable params: {trainable:.2f}M")
    
    # æ£€æŸ¥å…³é”®æ¨¡å—
    assert hasattr(model, 'vision_encoder'), "Missing vision_encoder"
    assert hasattr(model, 'vision_proj'), "Missing vision_proj"
    
    print(f"âœ“ Key modules verified")
    print("âœ“ Step 3.1 PASSED")
    
except Exception as e:
    print(f"âœ— Step 3.1 FAILED: {e}")
    import traceback
    traceback.print_exc()
```

è¿è¡Œ:
```bash
cd notes
python test_model_instance.py
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| `ImportError` | å¯¼å…¥è·¯å¾„é”™è¯¯ | æ£€æŸ¥ `sys.path.insert(0, '..')` |
| `RuntimeError: CUDA out of memory` | æ˜¾å­˜ä¸è¶³ | ä½¿ç”¨ CPU æ¨¡å¼æˆ–å‡å°æ¨¡å‹è§„æ¨¡ |
| `FileNotFoundError` | æƒé‡è·¯å¾„é”™è¯¯ | æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡® |
| `TypeError` | æ–¹æ³•ç­¾åä¸åŒ¹é… | æ£€æŸ¥ `__init__` å‚æ•° |

---

### Step 3.2: æµ‹è¯•å‰å‘ä¼ æ’­

åˆ›å»ºæµ‹è¯•è„šæœ¬ `notes/test_forward_pass.py`:

```python
import torch
import sys
sys.path.insert(0, '..')
from model.model_vlm import MiniMindVLM, VLMConfig
from transformers import AutoTokenizer

print("="*50)
print("Step 3.2: æµ‹è¯•å‰å‘ä¼ æ’­")
print("="*50)

try:
    config = VLMConfig(hidden_size=1024, num_hidden_layers=28)
    model = MiniMindVLM(
        params=config,
        vision_model_path="../model/vision_model/clip-vit-base-patch16",
        qwen_weight_path="../Models/Qwen3-0.6B"
    )
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained("../Models/Qwen3-0.6B")
    
    # æµ‹è¯•çº¯æ–‡æœ¬æ¨¡å¼
    print("\n[æµ‹è¯• 1] çº¯æ–‡æœ¬å‰å‘ä¼ æ’­...")
    text_input = "Hello, how are you?"
    inputs = tokenizer(text_input, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask']
        )
    
    print(f"  âœ“ Text mode works")
    print(f"    - Logits shape: {outputs.logits.shape}")
    print(f"    - Loss: {outputs.loss}")
    
    # æµ‹è¯• multimodal æ¨¡å¼
    print("\n[æµ‹è¯• 2] Multimodal å‰å‘ä¼ æ’­...")
    from PIL import Image
    import numpy as np
    
    # å‡†å¤‡å›¾åƒå ä½ç¬¦ (196ä¸ª @ ç¬¦å·)
    image_placeholder = '@' * 196
    prompt = f"{image_placeholder}\nWhat is in this image?"
    
    inputs = tokenizer(prompt, return_tensors="pt", max_length=256, truncation=True, padding='max_length')
    
    # æ¨¡æ‹Ÿå›¾åƒ
    dummy_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    image = Image.fromarray(dummy_image)
    pixel_values = model.processor(images=image, return_tensors="pt")['pixel_values']
    # æ‰©å±•ä¸º [1, 1, 3, 224, 224]
    pixel_values = pixel_values.unsqueeze(1)
    
    # ç®€åŒ–çš„ forward æµ‹è¯•
    print(f"  âœ“ Multimodal mode setup complete")
    print(f"    - Input IDs shape: {inputs['input_ids'].shape}")
    print(f"    - Pixel values shape: {pixel_values.shape}")
    
    print("âœ“ Step 3.2 PASSED")
    
except Exception as e:
    print(f"âœ— Step 3.2 FAILED: {e}")
    import traceback
    traceback.print_exc()
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `ValueError` | è¾“å…¥å½¢çŠ¶ä¸åŒ¹é… | æ£€æŸ¥ `pixel_values` çš„å½¢çŠ¶ |
| `RuntimeError` | forward å‚æ•°é”™è¯¯ | æ£€æŸ¥ `forward` æ–¹æ³•ç­¾å |
| æƒé‡åŠ è½½ä¸å®Œæ•´ | safetensors éƒ¨åˆ†å¼ é‡ç¼ºå¤± | æ£€æŸ¥ `load_state_dict` çš„è¾“å‡º |

---

## é˜¶æ®µå››: è®­ç»ƒé€»è¾‘é€‚é…

### Step 4.1: ä¿®æ”¹ trainer_utils.py

ç¼–è¾‘ `trainer/trainer_utils.py`ï¼Œæ‰¾åˆ° `init_vlm_model` å‡½æ•° (66-93 è¡Œ):

**å®Œæ•´æ›¿æ¢ä¸º**:
```python
def init_vlm_model(vlm_config, from_weight='pretrain_vlm', 
                   tokenizer_path='../Models/Qwen3-0.6B',
                   vision_model_path='../model/vision_model/clip-vit-base-patch16', 
                   save_dir='../out', device='cuda', freeze_llm=False):
    from transformers import AutoTokenizer
    import os
    
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    model = MiniMindVLM(vlm_config, vision_model_path=vision_model_path,
                       qwen_weight_path=tokenizer_path)
    
    if from_weight != 'none':
        if from_weight.endswith('.safetensors'):
            from safetensors import safe_open
            state_dict = {}
            weight_path = from_weight if os.path.isabs(from_weight) else os.path.join(save_dir, from_weight)
            with safe_open(weight_path, framework='pt', device=device) as f:
                for key in f.keys():
                    state_dict[key] = f.get_tensor(key)
            model.load_state_dict(state_dict, strict=False)
        else:
            moe_suffix = '_moe' if vlm_config.use_moe else ''
            weight_path = f'{save_dir}/{from_weight}_{vlm_config.hidden_size}{moe_suffix}.pth'
            if os.path.exists(weight_path):
                weights = torch.load(weight_path, map_location=device)
                model.load_state_dict(weights, strict=False)
            else:
                print(f"[Warning] Weight file not found: {weight_path}")
    
    if freeze_llm:
        for name, param in model.named_parameters():
            if 'vision_proj' not in name:
                param.requires_grad = False
    
    get_model_params(model, vlm_config)
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    print(f'Trainable Params: {trainable:.3f}M')
    preprocess = model.processor
    return model.to(device), tokenizer, preprocess
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `NameError: name 'MiniMindVLM' is not defined` | æ£€æŸ¥å¯¼å…¥è¯­å¥ |
| `OSError` | safetensors æ–‡ä»¶ä¸å­˜åœ¨ | è·³è¿‡åŠ è½½æˆ–æä¾›æ­£ç¡®è·¯å¾„ |
| å†»ç»“å‚æ•°é—®é¢˜ | `requires_grad` è®¾ç½®é”™è¯¯ | æ‰“å° `model.named_parameters()` æ£€æŸ¥ |

---

### Step 4.2: è°ƒæ•´è®­ç»ƒå‚æ•°

ç¼–è¾‘è®­ç»ƒè„šæœ¬å‚æ•°:

**trainer/train_pretrain_vlm.py** å’Œ **trainer/train_sft_vlm.py**:

æ‰¾åˆ° `parser.add_argument` éƒ¨åˆ†ï¼Œä¿®æ”¹ä»¥ä¸‹å‚æ•°:

```python
# åŸå‚æ•°
parser.add_argument('--hidden_size', default=512, type=int)
parser.add_argument('--num_hidden_layers', default=8, type=int)
parser.add_argument('--batch_size', default=16, type=int)

# ä¿®æ”¹ä¸º
parser.add_argument('--hidden_size', default=1024, type=int)
parser.add_argument('--num_hidden_layers', default=28, type=int)
parser.add_argument('--batch_size', default=2, type=int)  # é™ä½ä»¥é€‚åº”æ˜¾å­˜
parser.add_argument('--learning_rate', default=2e-4, type=float)  # é™ä½å­¦ä¹ ç‡
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `CUDA out of memory` | æ˜¾å­˜ä¸è¶³ | é™ä½ batch_size æˆ–å¯ç”¨ gradient checkpointing |
| Loss éœ‡åŠ¨æˆ– NaN | å­¦ä¹ ç‡è¿‡é«˜ | é™ä½ `learning_rate` æˆ–ä½¿ç”¨ warmup |

---

### Step 4.3: æ·»åŠ  gradient checkpointing (å¯é€‰)

åœ¨ `model_vlm.py` çš„ `MiniMindVLM.__init__` ä¸­æ·»åŠ :

```python
def __init__(self, ...):
    # ... å…¶ä»–åˆå§‹åŒ–
    
    # å¯ç”¨ gradient checkpointing ä»¥èŠ‚çœæ˜¾å­˜
    self.gradient_checkpointing_enable()
```

æˆ–è€…åœ¨è®­ç»ƒè„šæœ¬ä¸­è®¾ç½®:

```python
model.gradient_checkpointing_enable()
```

---

## é˜¶æ®µäº”: é¦–æ¬¡æµ‹è¯•è¿è¡Œ

### Step 5.1: åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®

åˆ›å»ºæµ‹è¯•è„šæœ¬ `notes/test_training_minimal.py`:

```python
import torch
import sys
sys.path.insert(0, '..')
from transformers import AutoTokenizer
from model.model_vlm import MiniMindVLM, VLMConfig

print("="*50)
print("Step 5.1: æœ€å°è®­ç»ƒæµç¨‹æµ‹è¯•")
print("="*50)

try:
    config = VLMConfig(
        hidden_size=1024,
        num_hidden_layers=28,
        max_seq_len=512,
        use_moe=False
    )
    
    model = MiniMindVLM(
        params=config,
        vision_model_path="../model/vision_model/clip-vit-base-patch16",
        qwen_weight_path="../Models/Qwen3-0.6B"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("../Models/Qwen3-0.6B")
    
    # å‡†å¤‡æ¨¡æ‹Ÿæ•°æ®
    batch_size = 2
    seq_len = 128
    
    input_ids = torch.randint(0, 100000, (batch_size, seq_len))
    labels = input_ids.clone()
    labels[:, -1] = -100  # mask last token
    
    # æ¨¡æ‹Ÿå›¾åƒ
    from PIL import Image
    import numpy as np
    dummy_img = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    img_tensors = model.processor(images=[Image.fromarray(dummy_img) for _ in range(batch_size)], 
                                  return_tensors="pt")['pixel_values']
    img_tensors = img_tensors.unsqueeze(1).unsqueeze(2)  # [bs, 1, 1, 3, 224, 224]
    
    print(f"\nè¾“å…¥å‡†å¤‡å®Œæˆ:")
    print(f"  - input_ids: {input_ids.shape}")
    print(f"  - labels: {labels.shape}")
    print(f"  - pixel_values: {img_tensors.shape}")
    
    # è®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    print(f"\nå¼€å§‹è®­ç»ƒæ¨¡æ‹Ÿ...")
    for step in range(3):
        optimizer.zero_grad()
        
        outputs = model(
            input_ids=input_ids,
            labels=labels,
            pixel_values=img_tensors
        )
        
        loss = outputs.loss + outputs.aux_loss
        loss.backward()
        optimizer.step()
        
        print(f"  Step {step+1}: loss={loss.item():.4f}, aux_loss={outputs.aux_loss.item():.4f}")
    
    print(f"\nâœ“ è®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡!")
    print("âœ“ Step 5.1 PASSED")
    
except Exception as e:
    print(f"âœ— Step 5.1 FAILED: {e}")
    import traceback
    traceback.print_exc()
```

è¿è¡Œ:
```bash
cd notes
python test_training_minimal.py
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `RuntimeError: shape mismatch` | å¼ é‡å½¢çŠ¶ä¸åŒ¹é… | æ£€æŸ¥ `pixel_values` çš„å½¢çŠ¶ (åº”ä¸º 6 ç»´) |
| `ValueError` | labels å½¢çŠ¶é—®é¢˜ | ç¡®ä¿ä¸ `input_ids` å½¢çŠ¶ä¸€è‡´ |
| `CUDA out of memory` | æ˜¾å­˜ä¸è¶³ | ä½¿ç”¨ CPU æˆ–å‡å° batch_size |

---

### Step 5.2: å°è§„æ¨¡é¢„è®­ç»ƒæµ‹è¯•

```bash
# ä½¿ç”¨å°‘é‡æ•°æ®æµ‹è¯•é¢„è®­ç»ƒ
cd ..
python trainer/train_pretrain_vlm.py \
    --epochs 1 \
    --batch_size 1 \
    --data_path ../dataset/pretrain_i2t.parquet \
    --save_weight test_pretrain \
    --log_interval 10 \
    --save_interval 100 \
    --from_weight none \
    --freeze_llm 1
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `FileNotFoundError` | æ•°æ®é›†ä¸å­˜åœ¨ | æ£€æŸ¥ `../dataset/pretrain_i2t.parquet` è·¯å¾„ |
| æ•°æ®åŠ è½½å¤±è´¥ | æ•°æ®æ ¼å¼é—®é¢˜ | æ£€æŸ¥ VLMDataset ç±»æ˜¯å¦æ­£ç¡®å¤„ç†æ•°æ® |
| Loss ä¸ä¸‹é™ | å­¦ä¹ ç‡è¿‡é«˜/è¿‡ä½ | è°ƒæ•´ `--learning_rate` |

---

## é˜¶æ®µå…­: æ­£å¼è®­ç»ƒ

### Step 6.1: å®Œæ•´é¢„è®­ç»ƒ

```bash
python trainer/train_pretrain_vlm.py \
    --epochs 4 \
    --batch_size 2 \
    --learning_rate 2e-4 \
    --data_path ../dataset/pretrain_i2t.parquet \
    --save_weight pretrain_qwen3_vlm \
    --log_interval 100 \
    --save_interval 1000 \
    --from_weight none \
    --freeze_llm 1 \
    --use_wandb
```

**å‚æ•°è¯´æ˜**:
| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `--epochs` | 4 | è®­ç»ƒè½®æ•° |
| `--batch_size` | 2 | æ‰¹æ¬¡å¤§å° (æ ¹æ®æ˜¾å­˜è°ƒæ•´) |
| `--learning_rate` | 2e-4 | å­¦ä¹ ç‡ |
| `--freeze_llm` | 1 | å†»ç»“ LLMï¼Œåªè®­ç»ƒ projector |

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| è®­ç»ƒé€Ÿåº¦æ…¢ | æ•°æ®åŠ è½½ç“¶é¢ˆ | å¢åŠ  `--num_workers` |
| Loss æ³¢åŠ¨å¤§ | å­¦ä¹ ç‡ä¸ç¨³å®š | æ·»åŠ  warmup scheduler |
| æ˜¾å­˜ä¸è¶³ | æ¨¡å‹å¤ªå¤§ | ä½¿ç”¨ gradient checkpointing æˆ– DeepSpeed |

---

### Step 6.2: SFT å¾®è°ƒ

```bash
python trainer/train_sft_vlm.py \
    --epochs 2 \
    --batch_size 1 \
    --learning_rate 1e-5 \
    --data_path ../dataset/sft_i2t.parquet \
    --save_weight sft_qwen3_vlm \
    --from_weight pretrain_qwen3_vlm \
    --log_interval 50 \
    --save_interval 500 \
    --use_wandb
```

**å¯èƒ½é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| æ— æ³•åŠ è½½é¢„è®­ç»ƒæƒé‡ | checkpoint æ–‡ä»¶åé”™è¯¯ | æ£€æŸ¥ `../out/pretrain_qwen3_vlm_1024.pth` |
| SFT æ•°æ®æ ¼å¼ä¸ä¸€è‡´ | å­—æ®µåç§°é”™è¯¯ | æ£€æŸ¥æ•°æ®é›†çš„ `conversations` æ ¼å¼ |
| è®­ç»ƒä¸ç¨³å®š | å­¦ä¹ ç‡è¿‡é«˜ | é™ä½åˆ° `1e-6` |

---

### Step 6.3: æµ‹è¯•æ¨ç†

åˆ›å»ºæµ‹è¯•è„šæœ¬ `notes/test_inference.py`:

```python
import torch
import sys
sys.path.insert(0, '..')
from transformers import AutoTokenizer
from model.model_vlm import MiniMindVLM, VLMConfig

print("="*50)
print("Step 6.3: æ¨ç†æµ‹è¯•")
print("="*50)

try:
    config = VLMConfig(hidden_size=1024, num_hidden_layers=28)
    model = MiniMindVLM(params=config)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    checkpoint = torch.load("../out/sft_qwen3_vlm_1024.pth", map_location="cpu")
    model.load_state_dict(checkpoint, strict=False)
    model.eval()
    model.cuda()
    
    tokenizer = AutoTokenizer.from_pretrained("../Models/Qwen3-0.6B")
    
    # å‡†å¤‡è¾“å…¥
    from PIL import Image
    import numpy as np
    
    test_image_path = "../dataset/eval_images/åŸå¸‚è½¦æ°´é©¬é¾™-city-traffic.jpg"
    if os.path.exists(test_image_path):
        image = Image.open(test_image_path).convert("RGB")
    else:
        image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
    
    image_placeholder = '@' * 196
    prompt = f"{image_placeholder}\nè¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆå†…å®¹?"
    
    inputs = tokenizer(prompt, return_tensors="pt")
    pixel_values = model.processor(images=image, return_tensors="pt")['pixel_values']
    pixel_values = pixel_values.unsqueeze(1).unsqueeze(2)
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs['input_ids'].cuda(),
            pixel_values=pixel_values.cuda(),
            max_new_tokens=100,
            temperature=0.7
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nç”¨æˆ·: {prompt.replace(image_placeholder, '<image>')}")
    print(f"æ¨¡å‹: {response}")
    
    print("âœ“ Step 6.3 PASSED")
    
except Exception as e:
    print(f"âœ— Step 6.3 FAILED: {e}")
    import traceback
    traceback.print_exc()
```

---

## æ•…éšœæ’æŸ¥æ±‡æ€»

### A. æƒé‡å’ŒåŠ è½½é—®é¢˜

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| load_state_dict missing keys | ç»´åº¦ä¸åŒ¹é… | ä½¿ç”¨ `strict=False` |
| safetensors åŠ è½½å¤±è´¥ | æ–‡ä»¶æŸå | é‡æ–°ä¸‹è½½æƒé‡ |
| Vision Encoder æƒé‡å¤ä½ | å‚æ•°åå˜åŒ– | æ‰“å° `æ¨¡å‹.state_dict().keys()` å¯¹ç…§ |

---

### B. å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| pixel_values å½¢çŠ¶é”™è¯¯ | åº”ä¸º `[bs, num_imgs, 1, 3, 224, 224]` |
| hidden_states ç»´åº¦é”™ | æ£€æŸ¥ `hidden_size=1024` |
| vision_tokens æ•°é‡é”™ | CLIP è¾“å‡ºåº”ä¸º 196 ä¸ª tokens |

---

### C. æ˜¾å­˜é—®é¢˜

```python
# æ–¹æ¡ˆ 1: å¯ç”¨ gradient checkpointing
model.gradient_checkpointing_enable()

# æ–¹æ¡ˆ 2: é™ä½ batch_size + æ¢¯åº¦ç´¯ç§¯
--batch_size 1 --accumulation_steps 8

# æ–¹æ¡ˆ 3: ä½¿ç”¨ DeepSpeed
deepspeed config.json train_pretrain_vlm.py \
    --deepspeed ds_config.json

# æ–¹æ¡ˆ 4: æ··åˆç²¾åº¦è®­ç»ƒ
--dtype bfloat16
```

---

### D. è®­ç»ƒä¸ç¨³å®šé—®é¢˜

| ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| Loss æŒ¯è¡ | é™ä½å­¦ä¹ ç‡ï¼Œæ·»åŠ  warmup |
| Loss NaN | æ£€æŸ¥æ¢¯åº¦è£å‰ª `--grad_clip 1.0` |
| æŸå¤±ä¸ä¸‹é™ | å¢åŠ  `epochs` æˆ–è°ƒæ•´æ•°æ® |

---

### E. æ•°æ®é—®é¢˜

```python
# æ£€æŸ¥æ•°æ®é›†æ ¼å¼
import pandas as pd
df = pd.read_parquet("../dataset/pretrain_i2t.parquet")
print(df.head())
print(df.columns)

# æ£€æŸ¥å›¾åƒå ä½ç¬¦
print(df.iloc[0]['conversations'])
# åº”åŒ…å«å›¾åƒ token æ ‡è®°
```

---

## é™„å½•: å¿«é€Ÿè¯Šæ–­æ¸…å•

è¿è¡Œ `notes/diagnose.py` å¿«é€Ÿæ£€æŸ¥:

```python
import torch
import sys
sys.path.insert(0, '..')
from transformers import AutoTokenizer
from model.model_vlm import MiniMindVLM, VLMConfig
from safetensors import safe_open
import os

print("=== MiniMind-V æ”¹é€ è¯Šæ–­ ===\n")

# 1. æ£€æŸ¥ä¾èµ–
print("[1] æ£€æŸ¥ä¾èµ–...")
try:
    print(f"  torch: {torch.__version__}")
    print(f"  cuda: {torch.cuda.is_available()}")
    print(f"  transformers: {...}")
    print(f"  safetensors: {...}")
except Exception as e:
    print(f"  âœ— ä¾èµ–é—®é¢˜: {e}")

# 2. æ£€æŸ¥æƒé‡æ–‡ä»¶
print("\n[2] æ£€æŸ¥æƒé‡æ–‡ä»¶...")
paths = [
    "../Models/Qwen3-0.6B/model.safetensors",
    "../model/vision_model/clip-vit-base-patch16/config.json",
]
for path in paths:
    status = "âœ“" if os.path.exists(path) else "âœ—"
    print(f"  {status} {path}")

# 3. æµ‹è¯•æ¨¡å‹åŠ è½½
print("\n[3] æµ‹è¯•æ¨¡å‹åŠ è½½...")
try:
    config = VLMConfig(hidden_size=1024, num_hidden_layers=28)
    model = MiniMindVLM(params=config)
    print(f"  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
except Exception as e:
    print(f"  âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

# 4. æµ‹è¯•å‰å‘ä¼ æ’­
print("\n[4] æµ‹è¯•å‰å‘ä¼ æ’­...")
try:
    tokenizer = AutoTokenizer.from_pretrained("../Models/Qwen3-0.6B")
    inputs = tokenizer("test", return_tensors="pt")
    with torch.no_grad():
        outputs = model(input_ids=inputs['input_ids'])
    print(f"  âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"  è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
except Exception as e:
    print(f"  âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")

print("\n=== è¯Šæ–­å®Œæˆ ===")
```

---

**ç¥ä½ æ”¹é€ é¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ `01_detailed_modification_guide.md` è·å–æ›´å¤šç»†èŠ‚ã€‚**
